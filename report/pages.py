import pandas as pd
import plotly.express as px
import streamlit as st


def approach() -> None:
    st.header("Our approach")
    pd.options.plotting.backend = "plotly"
    st.markdown("""
        Our solution belongs to a generative adversarial network class. The solution consists of three networks:
        - `netG` - neural network acting as a generator. This network is inspired with VGG-19 architecture. It consists of 5831043 parameters (about 23.32 MB).
    """)
    with st.expander("Generator graph"):
        with open('report/images/Generator.png', 'rb') as file:
            st.download_button("Download netG.png", data=file, file_name="netG.png", mime='image/png')
            st.image('report/images/Generator.png')
    st.markdown("""
        - `netGD` - neural network acting as a global discriminator evaluating whether the whole image seems to be real. This network consists of 3 identical (in terms of the architecture, not parameters values) branches. Each branch extracts 256 features from the input image - it is done with convolutional layers (mixed with ReLU and local maximum pooling) and channel-wise global pooling with maxiumim function at the end of each branch. Later on attention mechanism is applied to these features. The network is parametrized by 3715905 values (about 14.86 MB).
    """)
    with st.expander("Global discriminator graph"):
        with open('report/images/GlobalDiscriminator.png', 'rb') as file:
            st.download_button("Download netGD.png", data=file, file_name="netGD.png", mime='image/png')
            st.image('report/images/GlobalDiscriminator.png')
    st.markdown("""
        - `netLD` - neural network acting as a local discriminator evaluating whether the erased area seems to be real. It is done by expansion of channels to 64. It's performed only on the snippet, not the whole image. After on each channel three global poolings are applied using `min`, `mean` and `max` functions. This way 192 values are obtained. They are passed through linear layers and that's how predictions are obtained. This network consists of 118785 weights (about 0.47 MB).
    """)
    with st.expander("Local discriminator graph"):
        with open('report/images/LocalDiscriminator.png', 'rb') as file:
            st.download_button("Download netLD.png", data=file, file_name="netLD.png", mime='image/png')
            st.image('report/images/LocalDiscriminator.png')   
    
    st.header("Final implementation")
    st.markdown("""
        GAN training is a complex process where generator and discriminators are constantly trying to get better than the other. In such environment, it often occur that one of the partial models becomes too accurate compared to its counterpart, which results in the other one unable to learn anything useful. 
    """)
    st.subheader("Stage 1")
    st.markdown("""
        During our initial experiments, the situation described above occurred frequently – discriminators would discriminate almost perfectly from the start, which resulted in generator learning nothing but lots of random noise. In order to combat this problem, we have utilised several modifications to the learning process, hoping this would be enough to close the gap in the performance of different parts of the model.

        We have introduced a two stage learning process – for the first few epochs, the only component being trained is the generator. Due to a lack of discriminators in this stage, their loss cannot be utilised; thus the only component contributing to the loss used during this training round is pixel-wise MSE (Mean Squared Error) between target image and output of the generator. Typically, the time in this stage used by us was set to 10 epochs. 
    """)
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Stage 2")
        st.markdown("""
            The second stage of training introduced possibility of using local and global discriminators during learning; however, the influence of a given discriminator can be easily deactivated by changing the parameters of the run. By default, both discriminators use sum of BCE (Binary Cross Entropy) loss between discriminator’s output for real image and tensor symbolising “True” predictions and BCE loss between output for image generated by the generator and the tensor symbolising “False” prediction. 

            The loss used to train generator is more complex as it combines three components: pixel-wise MSE loss used in the previous stage and BCE losses between the outputs of local and global discriminators for generated images and tensor symbolising “True” prediction. It is important to mention that local discriminator provided training signals (loss gradients) only for the missing region, whereas global discriminator back-propagates loss gradients across the entire image. In order to balance the impact of various components of generator loss, each of them is multiplied by associated weight. The general formula for generator loss can be presented as:
        """)
        st.latex(r"L_G = \lambda_{MSE} * L_{MSE} + \lambda_{GD} * L_{GD} + \lambda_{LD} * L_{LD}")  

        st.markdown("""
            While the generator loss based on performance of discriminators is used for training GAN-based architecture, the values it yields depend on the performance of discriminators and, as such, they are difficult to interpret correctly. Therefore, during evaluation of our models, we are using pixel-wise MSE and MAE losses calculated between target images and the results generated by the generator. This evaluation metric remains the same between stage 1 and stage 2 and provides information independent of the discriminators.
        """)
    with col2:
        df_lr = pd.read_csv('report/dataframes/stage2_bce.csv')
        fig = df_lr.plot()
        fig.update_xaxes(title="epoch")
        fig.update_yaxes(title="binary cross entropy")
        st.plotly_chart(fig)
                
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Architecture tuning")
        st.markdown("""            
            We have tested three approaches of fitting genrator with different training strategies:
            - G - training with mean squared error as a criterion (only stage 1)
            - G&GD - after training in stage 1 the geneator is fitted using weighted sum of mean squared error and binary cross entropy of classification performed by the global discriminator
            - G&GD&LD - in the criterion there is one more element - binary cross entropy of classification done by the local discriminator
        """)
        st.dataframe(pd.DataFrame(data={"Training time": ["14min 30s", "1h 20min", "1h 32min"]}, index=["G", "G&GD", "G&GD&LD"]))
    with col2:
        df_lr = pd.read_csv('report/dataframes/final_networks.csv')
        fig = df_lr.plot()
        fig.update_xaxes(title="epoch")
        fig.update_yaxes(title="mean absolute error")
        fig.add_vline(x=9.0)
        st.plotly_chart(fig)
        
    st.header("Inference")
    col1, col2, col3 = st.columns(3)
    col1.markdown("Inference is performed using only the generator. Hence inference time is independent from training approach and equals about 75 milliseconds. Some exemplary results can be observed beside this text. If you would like to perform inference on your own visit page \"Live demo\" (see menu on the left-hand-side.")
    col2.image('report/images/infer1.png')
    col3.image('report/images/infer2.png')


def bibliography() -> None:
    st.header("Sources")
    st.markdown("""
    - Generative Face Completion, Yijun Li, Sifei Li, Jimei Yang, Ming-Hsuan Yang, 2017, ([arXiv](https://arxiv.org/pdf/1704.05838.pdf)) - inspiration for a general GAN framework
    - Very Deep Convolutional Networks for Large-Scale Image Recognition, Karen Simonyan, Andrew Zisserman, 2015 ([arXiv](https://arxiv.org/pdf/1409.1556.pdf)) - inspiration for the generator architecture (VGG-19)
    - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, Alec Radford, Luke Metz, Soumith Chintala, 2016, ([arXiv](https://arxiv.org/pdf/1511.06434.pdf), [PyTorch](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)) - inspiration for the training loop
    """)


def environment() -> None:
    st.header("Environment used for conducting experiments")
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Hardware")
        st.markdown(r"""
            - CPU
                - AMD Ryzen 5 3600X
                - Base clock speed - 3.8 GHz
                - 12 threads
                - Cache 384KB/3MB/32MB
            - RAM
                - 16 GB
                - Memory speed - 3200MHz
            - GPU
                - NVIDIA GeForce GTX960
                - 4 GB RAM
                - 1024 CUDA cores
                - Base clock speed - 1178 MHz
        """)
        
    with col2:
        st.subheader("Software")
        st.markdown("""
            - Ubuntu 20.04
            - CUDA Toolkit 11.5
            - neptune.ai - [experiments tracking](https://share.streamlit.io/jkarolczak/image-inpainting/main/report/app.py)
            - DVC
            - Python 3.9 with the following packages:
                - `PyTorch` (with `cudatoolkit`) - implementation of neural networks
                - `numpy` - generating dataset, manipulating the data
                - `pandas` - managing datasets
                - `opencv` - manipulating pictures, reading and writing images to files
                - `scikit-learn` - splitting dataset
                - `streamlit` - deploying the final model in an accesbile way for end-users and writing this report
                - `neptune-client` - tracking experiments using [neptune.ai](https://neptune.ai/)
                - `matplotlib.pyplot` - visualizing results 
        """)    
        
        
def hyperparameters() -> None:
    st.header("Hyperparameters")
    st.markdown("""
    Transfering all of the parameters used by our models to separate .yaml files leads to much easier management and monitoring of settings used in every recorded run. Moreover, correctly defined stages in DVC pipeline can store this data and use it to verify whether it should run a particular step in the pipeline. 
    Parameters have been distributed across several smaller files in order to provide readability and separation of information:
     - `dataset.yaml > generate` – contains parameters of dataset generation pipeline stage:
        - seed – seed used by the random number generator when selecting the images from the entire dataset and when selecting the region to mask
        - dataset_size – the desired number of instances in generated dataset
     - `training.yaml > dataloader` – contains parametes of dataloader during training (batch size, number of workers and whether the input images should be shuffled)
     - `training.yaml > stage1` – defines the parameters used during stage 1 of the training:
        - loss – loss used to train the generator (default: “mse”)
        - epochs – number of epochs that should be dedicated to this stage
        - limit_iters – specifies the number of minibatches that should be used during each epoch of the training
        - netG – defined the parameters connected specifically with the training of generator:
            - optimizer – optimizer used, by default “adam” is used
            - lr – learning rate
            - weight_decay – weight_decay used when using “adamW” optimizer
     - `training.yaml > stage2` – defines the parameters used during stage 2 of the training:
        - discriminator_loss – loss used to train the discriminators (default: “bce”)
        - generator_loss – loss used to train the generator (default: “mse”)
        - epochs – number of epochs that should be dedicated to this stage
        - limit_iters - specifies the number of minibatches that should be used during each epoch of the training
        - netLD and netGD – specifies the parameters connected specifically with the training of (respectively) local and global discriminators; the same as in stage2 with addition of:
            - train_every – allows training the model for one epoch every n epochs
        - netG - specifies the parameters connected specifically with the training of the generator; uses the same parameters as netLD and netGD with addition of:
            - mse_weight – weight assigned to MSE part in loss formula
            - local_weight – weight assigned to local discriminator’s part in loss
            - global_weight – weight assigned to global discriminator’s part in loss            
    """)
    st.header("Chosen values")
    st.markdown("""
    Due to some limitations and constraints we have chosen the following values of hyperparameters based on:
    - batchsize = 4 - GPU RAM
    - limit_iters = 250 - the timeframe for implementation and requirement of minimal number of instances (250 * 4 = 1000)
    - epochs = 10 (stage 1) + 20 (stage 2) - the timeframe for implementation
    
    We've performed a few experiments to choose best values of hyperparameters. Their results are available on the charts below.
    - learning rate = 1e-4
    - optimizer = Adam        
    """)
    
    pd.options.plotting.backend = "plotly"

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Learning rate")
        df_lr = pd.read_csv('report/dataframes/lr.csv')
        fig = df_lr.plot()
        fig.update_xaxes(title="epoch")
        fig.update_yaxes(title="mean absolute error")
        st.plotly_chart(fig)
        
    with col2:
        st.subheader("Optimizers")
        df_lr = pd.read_csv('report/dataframes/optimizer.csv')
        fig = df_lr.plot()
        fig.update_xaxes(title="epoch")
        fig.update_yaxes(title="mean absolute error")
        st.plotly_chart(fig)
        st.markdown("*AdamW was examined with weight decay. Becuase of other limitations we decided to not apply weight decay.")
        
        
def problem() -> None:
    col1, col2 = st.columns(2)
    with col1:
        st.header("Problem and dataset")
        st.markdown("""
            We try to address problem of image inpainting, precisely face inpainting. We use [CelebFaces dataset](https://www.kaggle.com/jessicali9530/celeba-dataset) - it consists of 202,599 images of size 218x178 px. From this dataset we generate our own dataset by removing (or actually substituting pixels with `(255, 255, 255)`) an arbitrary area from each image. The area is an rectange of width and hight from range 25-55px (height and width may differ). To increase probability of covering a part of a face the area the mask is put only in a region apart 35px from vertical borders and 45px from horizontal borders.
        """)
    with col2:
        st.image('report/images/roi.png')
    
    st.subheader("Exemplary images")
    cols = st.columns(5)
    for idx in range(5):
        cols[idx].image(f'report/images/img{idx + 1}.jpg')
    